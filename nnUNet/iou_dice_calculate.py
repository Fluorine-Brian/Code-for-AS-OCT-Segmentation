import os
import json
import numpy as np
import SimpleITK as sitk
from tqdm import tqdm
from collections import OrderedDict
from multiprocessing import Pool
from typing import Tuple, Dict, List, Optional

# ----------------------------------------------------
# 1. é…ç½®è·¯å¾„ (æ ¹æ®æ‚¨çš„å®é™…è·¯å¾„è®¾ç½®)
# ----------------------------------------------------
# é¢„æµ‹ç»“æœè·¯å¾„ (æ¥è‡ª nnUNetv2_predict çš„è¾“å‡º)
PREDICTIONS_FOLDER = r"/home/fxy/nnUNet/nnUNetFrame/DATASET/nnUNet_results/test_results/Dataset800_AS-OCT"
# Ground Truth æ ‡ç­¾è·¯å¾„ (æ¥è‡ªæ‚¨åˆšæ‰ç”Ÿæˆçš„ labelsTs)
LABELS_GT_FOLDER = r"/home/fxy/nnUNet/nnUNetFrame/DATASET/nnUNet_raw/Dataset800_AS-OCT/labelsTs"

# ----------------------------------------------------
# 2. è¯„ä¼°å‚æ•°
# ----------------------------------------------------
# ç±»åˆ«åç§°ä¸æ ‡ç­¾ ID (1, 2, 3, 4, 5) å¯¹åº”
CLASSES_TO_EVALUATE = {
    1: "lens",
    2: "left_iris",
    3: "right_iris",
    4: "anterior_chamber",
    5: "nucleus"
}
CLASSES_TO_EVALUATE_REVERSE = {v: k for k, v in CLASSES_TO_EVALUATE.items()}

# ----------------------------------------------------
# 3. æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—å‡½æ•° (Dice å’Œ Jaccard/IoU)
# ----------------------------------------------------

def compute_metric(
    prediction_file: str, 
    gt_folder: str, 
    class_id: int, 
    class_name: str
) -> Optional[Tuple[str, int, float, float]]:
    """è®¡ç®—å•ä¸ªæ–‡ä»¶ã€å•ä¸ªç±»åˆ«çš„ Dice å’Œ Jaccard (IoU)"""
    try:
        # 1. è¯»å–é¢„æµ‹ç»“æœ
        pred_path = os.path.join(PREDICTIONS_FOLDER, prediction_file)
        pred_itk = sitk.ReadImage(pred_path)
        pred_np = sitk.GetArrayFromImage(pred_itk).astype(np.uint8)

        # 2. æ„é€  GT è·¯å¾„
        case_id = prediction_file.replace('.nii.gz', '')
        gt_path = os.path.join(gt_folder, f"{case_id}.nii.gz")
        
        if not os.path.exists(gt_path):
             # print(f"Warning: GT file not found for {case_id} at {gt_path}. Skipping.")
             return None

        # 3. è¯»å– Ground Truth
        gt_itk = sitk.ReadImage(gt_path)
        gt_np = sitk.GetArrayFromImage(gt_itk).astype(np.uint8)
        
        # ç¡®ä¿ GT å’Œ Prediction ç»´åº¦åŒ¹é…
        if gt_np.shape != pred_np.shape:
            # print(f"Warning: Shape mismatch for {case_id}. Skipping.")
            return None

        # 4. æå–å½“å‰ç±»åˆ«çš„äºŒå€¼æ©ç 
        pred_binary = (pred_np == class_id)
        gt_binary = (gt_np == class_id)
        
        # å¦‚æœ GT æˆ– Pred ä¸­å½“å‰ç±»åˆ«å®Œå…¨ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡
        if not gt_binary.any() and not pred_binary.any():
             # å¦‚æœä¸¤è€…éƒ½ç¼ºå¤±ï¼ŒDice/IoU å®šä¹‰ä¸º 1.0 (å®Œç¾åŒ¹é…)
             return case_id, class_id, 1.0, 1.0 
        
        # 5. è®¡ç®—äº¤é›†å’Œå¹¶é›†
        intersection = np.sum(pred_binary & gt_binary)
        union = np.sum(pred_binary | gt_binary)
        
        # 6. è®¡ç®— Dice å’Œ Jaccard (IoU)
        dice_score = (2.0 * intersection) / (np.sum(pred_binary) + np.sum(gt_binary))
        jaccard_score = intersection / union if union > 0 else 0.0

        return case_id, class_id, dice_score, jaccard_score

    except Exception as e:
        print(f"Error processing {prediction_file} for class {class_name}: {e}")
        return None


def run_evaluation():
    """ä¸»è¯„ä¼°é€»è¾‘ï¼Œæ”¶é›†å¹¶æ±‡æ€»ç»“æœ"""
    print(f"--- ğŸš€ å¼€å§‹è¯„ä¼° nnU-Netv2 ç»“æœ (å«æ–¹å·®å’Œæ ‡å‡†å·®) ---")

    # è·å–æ‰€æœ‰é¢„æµ‹æ–‡ä»¶ (.nii.gz æ–‡ä»¶)
    prediction_files = [f for f in os.listdir(PREDICTIONS_FOLDER) if f.endswith('.nii.gz')]
    if not prediction_files:
        print("é”™è¯¯ï¼šé¢„æµ‹æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°ä»»ä½• .nii.gz æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„æˆ–é¢„æµ‹æ˜¯å¦æˆåŠŸã€‚")
        return

    print(f"æ‰¾åˆ° {len(prediction_files)} ä¸ªé¢„æµ‹æ–‡ä»¶ã€‚")

    # æ„é€ å¤šè¿›ç¨‹ä»»åŠ¡åˆ—è¡¨
    tasks = []
    for filename in prediction_files:
        for class_id, class_name in CLASSES_TO_EVALUATE.items():
            tasks.append((filename, LABELS_GT_FOLDER, class_id, class_name))

    # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿè®¡ç®—
    num_processes = os.cpu_count() or 4
    results = []
    with Pool(num_processes) as p:
        results = list(tqdm(p.starmap(compute_metric, tasks), total=len(tasks), desc="è®¡ç®—æŒ‡æ ‡"))
    
    # è¿‡æ»¤æ‰ None çš„ç»“æœ
    results = [res for res in results if res is not None]

    # ----------------------------------------------------
    # 4. ç»“æœæ±‡æ€»ä¸å±•ç¤º (æ–°å¢æ–¹å·®å’Œæ ‡å‡†å·®)
    # ----------------------------------------------------
    
    # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„ Dice å’Œ IoU åˆ—è¡¨
    metrics_by_class: Dict[int, Dict[str, List[float]]] = {
        cid: {"Dice": [], "IoU": []} for cid in CLASSES_TO_EVALUATE.keys()
    }
    
    for case_id, class_id, dice, jaccard in results:
        metrics_by_class[class_id]["Dice"].append(dice)
        metrics_by_class[class_id]["IoU"].append(jaccard)

    # è®¡ç®—å¹³å‡å€¼ã€æ–¹å·®å’Œæ ‡å‡†å·®
    final_metrics = OrderedDict()
    
    for class_id, class_name in CLASSES_TO_EVALUATE.items():
        dice_scores = metrics_by_class[class_id]["Dice"]
        iou_scores = metrics_by_class[class_id]["IoU"]
        
        if dice_scores:
            final_metrics[class_name] = {
                "Mean Dice": np.mean(dice_scores),
                "Variance Dice": np.var(dice_scores),
                "Std Dice": np.std(dice_scores),
                "Mean IoU": np.mean(iou_scores),
                "Variance IoU": np.var(iou_scores),
                "Std IoU": np.std(iou_scores),
                "Cases": len(dice_scores)
            }

    # æ‰“å°ç»“æœ
    print("\n--- ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ (Mean, Std, Variance) ---")
    
    # æ‰“å°è¡¨å¤´
    header = f"{'Class':<20} | {'Mean Dice':<10} | {'Std Dice':<10} | {'Var Dice':<10} | {'Mean IoU':<10} | {'Std IoU':<10} | {'Var IoU':<10} | {'Cases':<5}"
    print("-" * len(header))
    print(header)
    print("-" * len(header))
    
    all_dice_scores = []
    
    for class_name, metrics in final_metrics.items():
        print(
            f"{class_name:<20} | "
            f"{metrics['Mean Dice']:.4f}  | "
            f"{metrics['Std Dice']:.4f}  | "
            f"{metrics['Variance Dice']:.4f}  | "
            f"{metrics['Mean IoU']:.4f}  | "
            f"{metrics['Std IoU']:.4f}  | "
            f"{metrics['Variance IoU']:.4f}  | "
            f"{metrics['Cases']:<5}"
        )
        all_dice_scores.append(metrics['Mean Dice'])
        
    # è®¡ç®—å¹³å‡ Dice (Mean Dice over all classes)
    if all_dice_scores:
        mean_dice_overall = np.mean(all_dice_scores)
        print("-" * len(header))
        print(f"{'Overall Mean Dice':<20} | {mean_dice_overall:.4f}")
    
    # å°†ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶
    output_json_path = os.path.join(PREDICTIONS_FOLDER, "evaluation_metrics_with_variance.json")
    with open(output_json_path, 'w') as f:
        json.dump(final_metrics, f, indent=4)
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_json_path}")
    
if __name__ == "__main__":
    # ç¡®ä¿ SimpleITK å’Œ numpy åœ¨å¤šè¿›ç¨‹ä¸­èƒ½æ­£ç¡®è¿è¡Œ
    sitk.ProcessObject.SetGlobalWarningDisplay(False)
    run_evaluation()